{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. 로지스틱 회귀(logistic Regression).ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNoxNbYBBIE2wqtMPHhPmWp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namsik-Yoon/pytorch_basic/blob/master/3.%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80(logistic_Regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0qodsM0uaxg"
      },
      "source": [
        "# 3. 로지스틱 회귀(Logistic Regression)\n",
        "\n",
        "일상 속 풀고자하는 많은 문제 중에서는 두 개의 선택지 중에서 정답을 고르는 문제가 많습니다. 예를 들어 시험을 봤는데 이 시험 점수가 합격인지 불합격인지가 궁금할 수도 있고, 어떤 메일을 받았을 때 이게 정상 메일인지 스팸 메일인지를 분류하는 문제도 그렇습니다. 이렇게 둘 중 하나를 결정하는 문제를 이진 분류(Binary Classification)라고 합니다. 그리고 이진 분류를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)가 있습니다.\n",
        "\n",
        "*   로지스틱 회귀는 알고리즘의 이름은 회귀이지만 실제로는 분류(Classification) 작업에 사용할 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N4QFwpj2iQr"
      },
      "source": [
        "## 3.1 로지스틱 회귀(Logistic Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0XVJBLumkU"
      },
      "source": [
        "### 3.1.1 이진 분류(Binary Classification)\n",
        "\n",
        "학생들이 시험 성적에 따라서 합격, 불합격이 기재된 데이터가 있다고 가정해봅시다. 시험 성적이 $x$라면, 합불 결과는 $y$입니다. 이 시험의 커트라인은 공개되지 않았는데 이 데이터로부터 특정 점수를 얻었을 때의 합격, 불합격 여부를 판정하는 모델을 만들고자 합시다.\n",
        "\n",
        "|score$(x)$|result$(y)$|\n",
        "|:----|:----|\n",
        "|45|불합격|\n",
        "|50|불합격|\n",
        "|55|불합격|\n",
        "|60|합격|\n",
        "|65|합격|\n",
        "|70|합격|\n",
        "\n",
        "위의 데이터에서 합격을 1, 불합격을 0이라고 하였을 때 그래프를 그려보면 아래와 같습니다.\n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80.PNG)\n",
        "\n",
        "이러한 점들을 표현하는 그래프는 알파벳의 S자 형태로 표현됩니다. 이러한 $x$와 $y$의 관계를 표현하기 위해서는 $Wx+b$와 같은 직선 함수가 아니라 S자 형태로 표현할 수 있는 함수가 필요합니다. 이런 문제에 직선을 사용할 경우 분류 작업이 잘 동작하지 않습니다.\n",
        "\n",
        "그래서 이번 로지스틱 회귀의 가설은 선형 회귀 때의 $H(x)=Wx+b$가 아니라, 위와 같이 S자 모양의 그래프를 만들 수 있는 어떤 특정 함수 $f$를 추가적으로 사용하여 $H(x)=f(Wx+b)$의 가설을 사용할 겁니다. 그리고 위와 같이 S자 모양의 그래프를 그릴 수 있는 어떤 함수 $f$가 이미 널리 알려져있습니다. 바로 시그모이드 함수입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI3fTHE3wCyE"
      },
      "source": [
        "### 3.1.2 시그모이드 함수(Sigmoid function)\n",
        "\n",
        "위와 같이 S자 형태로 그래프를 그려주는 시그모이드 함수의 방정식은 아래와 같습니다.\n",
        "\n",
        "$H(x) = sigmoid(Wx + b) = \\frac{1}{1 + e^{-(Wx + b)}} = \\sigma(Wx + b)$\n",
        "\n",
        "선형 회귀에서는 최적의 $W$와 $b$를 찾는 것이 목표였습니다. 여기서도 마찬가지입니다. 선형 회귀에서는 $W$가 직선의 기울기, $b$가 y절편을 의미했습니다. 그렇다면 여기에서는 $W$와 $b$가 함수의 그래프에 어떤 영향을 주는지 직접 그래프를 그려서 알아보겠습니다.\n",
        "\n",
        "\n",
        "*   파이썬에서는 그래프를 그릴 수 있는 도구로서 Matplotlib을 사용할 수 있습니다.\n",
        "\n",
        "우선 Matplotlib과 Numpy를 임포트합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG5VRXqyuly-"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np # 넘파이 사용\n",
        "import matplotlib.pyplot as plt # 맷플롯립사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTAXHGCzwXvt"
      },
      "source": [
        "Numpy를 사용하여 시그모이드 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgmw2m5MwWgN"
      },
      "source": [
        "def sigmoid(x): # 시그모이드 함수 정의\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMj2mF1wwceM"
      },
      "source": [
        "1) W가 1이고 b가 0인 그래프\n",
        "\n",
        "가장 먼저 $W$가 1이고, $b$가 0인 그래프를 그려봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKzWOynEwaCt"
      },
      "source": [
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.plot(x, y, 'g')\n",
        "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n",
        "plt.title('Sigmoid Function')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4waNup3qw2xb"
      },
      "source": [
        "위의 그래프를 통해시그모이드 함수는 출력값을 0과 1사이의 값으로 조정하여 반환함을 알 수 있습니다. $x$가 0일 때 0.5의 값을 가집니다. $x$가 매우 커지면 1에 수렴합니다. 반면, $x$가 매우 작아지면 0에 수렴합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0KwFbkw67z"
      },
      "source": [
        "2) W값의 변화에 따른 경사도의 변화\n",
        "\n",
        "이제 $W$의 값을 변화시키고 이에 따른 그래프를 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpuzxZt2w05d"
      },
      "source": [
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y1 = sigmoid(0.5*x)\n",
        "y2 = sigmoid(x)\n",
        "y3 = sigmoid(2*x)\n",
        "\n",
        "plt.plot(x, y1, 'r', linestyle='--') # W의 값이 0.5일때\n",
        "plt.plot(x, y2, 'g') # W의 값이 1일때\n",
        "plt.plot(x, y3, 'b', linestyle='--') # W의 값이 2일때\n",
        "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n",
        "plt.title('Sigmoid Function')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM0KYb0dxBqO"
      },
      "source": [
        "위의 그래프는 $W$의 값이 0.5일때 빨간색선, $W$의 값이 1일때는 초록색선, $W$의 값이 2일때 파란색선이 나오도록 하였습니다. 자세히 보면 $W$의 값에 따라 그래프의 경사도가 변하는 것을 볼 수 있습니다. 앞서 선형 회귀에서 가중치 $W$는 직선의 기울기를 의미했지만, 여기서는 그래프의 경사도를 결정합니다. $W$의 값이 커지면 경사가 커지고 $W$의 값이 작아지면 경사가 작아집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov-Fo4PmxIMt"
      },
      "source": [
        "3) b값의 변화에 따른 좌, 우 이동\n",
        "\n",
        "이제 $b$의 값에 따라서 그래프가 어떻게 변하는지 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2xKdHY7w_8k"
      },
      "source": [
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "y1 = sigmoid(x+0.5)\n",
        "y2 = sigmoid(x+1)\n",
        "y3 = sigmoid(x+1.5)\n",
        "\n",
        "plt.plot(x, y1, 'r', linestyle='--') # x + 0.5\n",
        "plt.plot(x, y2, 'g') # x + 1\n",
        "plt.plot(x, y3, 'b', linestyle='--') # x + 1.5\n",
        "plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가\n",
        "plt.title('Sigmoid Function')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9BHhfGRxOsW"
      },
      "source": [
        "위의 그래프는 $b$의 값에 따라서 그래프가 좌, 우로 이동하는 것을 보여줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y66KS3NwxdbO"
      },
      "source": [
        "4) 시그모이드 함수를 이용한 분류\n",
        "\n",
        "시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렴합니다. 시그모이드 함수의 출력값은 0과 1 사이의 값을 가지는데 이 특성을 이용하여 분류 작업에 사용할 수 있습니다. 예를 들어 임계값을 0.5라고 정해보겠습니다. 출력값이 0.5 이상이면 1(True), 0.5이하면 0(False)으로 판단하도록 할 수 있습니다. 이를 확률이라고 생각하면 해당 레이블에 속할 확률이 50%가 넘으면 해당 레이블로 판단하고, 해당 레이블에 속할 확률이 50%보다 낮으면 아니라고 판단하는 것으로 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_D3wwQ1xj2G"
      },
      "source": [
        "### 3.1.3 비용 함수(Cost function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP5Y9zRPxmh3"
      },
      "source": [
        "이제 로지스틱 회귀의 가설이 $H(x)=sigmoid(Wx+b)$인 것은 알았습니다. 이제 최적의 $W$와 $b$를 찾을 수 있는 비용 함수(cost function)를 정의해야 합니다. 그런데 혹시 앞서 선형 회귀에서 배운 비용 함수인 평균 제곱 오차(Mean Square Error, MSE)를 로지스틱 회귀의 비용 함수로 그냥 사용하면 안 될까요?\n",
        "\n",
        "다음은 선형 회귀에서 사용했던 평균 제곱 오차의 수식입니다.\n",
        "\n",
        "$cost(W, b) = \\frac{1}{n} \\sum_{i=1}^{n} \\left[y^{(i)} - H(x^{(i)})\\right]^2$\n",
        "\n",
        "위의 비용 함수 수식에서 가설은 이제 $H(x)=Wx+b$가 아니라 $H(x)=sigmoid(Wx+b)$입니다. 그리고 이 비용 함수를 미분하면 선형 회귀때와 달리 다음과 같이 비볼록(non-convex) 형태의 그래프가 나옵니다.\n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/22881/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG)\n",
        "\n",
        "위와 같은 그래프에 경사 하강법을 사용할 경우의 문제점은 경사 하강법이 오차가 최소값이 되는 구간에 도착했다고 판단한 그 구간이 실제 오차가 완전히 최소값이 되는 구간이 아닐 수 있다는 점입니다. 사람이 등산 후에 산을 내려올 때도, 가파른 경사를 내려오다가 넓은 평지가 나오면 순간적으로 다 내려왔다고 착각할 수 있습니다. 하지만 실제로는 그곳이 다 내려온 것이 아니라 잠깐 평지가 나왔을 뿐이라면 길을 더 찾아서 더 내려가야 할 겁니다. 모델도 마찬가지로 실제 오차가 최소가 되는 구간을 찾을 수 있도록 도와주어야 합니다. 만약, 실제 최소가 되는 구간을 잘못 판단하면 최적의 가중치 $W$가 아닌 다른 값을 택해 모델의 성능이 더 오르지 않습니다.\n",
        "\n",
        "이를 전체 함수에 걸쳐 최소값인 글로벌 미니멈(Global Minimum)이 아닌 특정 구역에서의 최소값인 로컬 미니멈(Local Minimum)에 도달했다고 합니다. 이는 cost가 최소가 되는 가중치 $W$를 찾는다는 비용 함수의 목적에 맞지 않습니다.\n",
        "\n",
        "시그모이드 함수의 특징은 함수의 출력값이 0과 1사이의 값이라는 점입니다. 즉, 실제값이 1일 때 예측값이 0에 가까워지면 오차가 커져야 하며, 실제값이 0일 때, 예측값이 1에 가까워지면 오차가 커져야 합니다. 그리고 이를 충족하는 함수가 바로 로그 함수입니다. 다음은 $y=0.5$에 대칭하는 두 개의 로그 함수 그래프입니다.\n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/57805/%EA%B7%B8%EB%9E%98%ED%94%84.PNG)\n",
        "\n",
        "실제값이 1일 때의 그래프를 주황색 선으로 표현하였으며, 실제값이 0일 때의 그래프를 초록색 선으로 표현하였습니다. 실제값이 1이라고 해봅시다. 이 경우, 예측값인 $H(x)$의 값이 1이면 오차가 0이므로 당연히 cost는 0이 됩니다. 반면, $H(x)$가 0으로 수렴하면 cost는 무한대로 발산합니다. 실제값이 0인 경우는 그 반대로 이해하면 됩니다. 이 두 개의 로그 함수를 식으로 표현하면 다음과 같습니다.\n",
        "\n",
        "$\\text{if } y=1 ,  \\text{cost}\\left( H(x), y \\right) = -\\log(H(x))$\n",
        "\n",
        "$\\text{if } y=0 , \\text{cost}\\left( H(x), y \\right) = -\\log(1-H(x))$\n",
        "\n",
        "$y$의 실제값이 1일 때 −logH(x) 그래프를 사용하고 y의 실제값이 0일 때 −log(1−H(X)) 그래프를 사용해야 합니다.\n",
        "이는 다음과 같이 하나의 식으로 통합할 수 있습니다.\n",
        "\n",
        "$\\text{cost}\\left( H(x), y \\right) = -[ylogH(x) + (1-y)log(1-H(x))]$\n",
        "\n",
        "왜 위 식이 두 개의 식을 통합한 식이라고 볼 수 있을까요? 실제값 $y$가 1이라고하면 덧셈 기호를 기준으로 우측의 항이 없어집니다. 반대로 실제값 $y$가 0이라고 하면 덧셈 기호를 기준으로 좌측의 항이 없어집니다. 선형 회귀에서는 모든 오차의 평균을 구해 평균 제곱 오차를 사용했었습니다. 마찬가지로 여기에서도 모든 오차의 평균을 구합니다.\n",
        "\n",
        "$cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]$\n",
        "\n",
        "정리하면, 위 비용 함수는 실제값 $y$와 예측값 $H(x)$의 차이가 커지면 cost가 커지고, 실제값 $y$와 예측값 $H(x)$의 차이가 작아지면 cost는 작아집니다. 이제 위 비용 함수에 대해서 경사 하강법을 수행하면서 최적의 가중치 $W$를 찾아갑니다.\n",
        "\n",
        "$W := W - \\alpha\\frac{\\partial}{\\partial W}cost(W)$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jozI9345zAnj"
      },
      "source": [
        "### 3.1.4 파이토치로 로지스틱 회귀 구현하기\n",
        "\n",
        "이제 파이토치로 로지스틱 회귀 중에서도 다수의 x로 부터 y를 예측하는 다중 로지스틱 회귀를 구현해봅시다.\n",
        "\n",
        "우선 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSRHxYETxNF7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6l1tDWazF0z"
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgPi3VFbzHgj"
      },
      "source": [
        "x_train과 y_train을 텐서로 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l43IJ7azGqH"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRD1OJ9szJ5g"
      },
      "source": [
        "앞서 훈련 데이터를 행렬로 선언하고, 행렬 연산으로 가설을 세우는 방법을 배웠습니다.\n",
        "여기서도 마찬가지로 행렬 연산을 사용하여 가설식을 세울겁니다. x_train과 y_train의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POwspkDczJHH"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rxd42SwzNTZ"
      },
      "source": [
        "현재 x_train은 6 × 2의 크기(shape)를 가지는 행렬이며, y_train은 6 × 1의 크기를 가지는 벡터입니다. x_train을 $X$라고 하고, 이와 곱해지는 가중치 벡터를 $W$라고 하였을 때, $XW$가 성립되기 위해서는 $W$ 벡터의 크기는 2 × 1이어야 합니다. 이제 W와 b를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H4XZWXszLqS"
      },
      "source": [
        "W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4DE77AQzWUL"
      },
      "source": [
        "이제 가설식을 세워보겠습니다. 파이토치에서는 $e^{x}$를 구현하기 위해서 torch.exp(x)를 사용합니다.\n",
        "이에 따라 행렬 연산을 사용한 가설식은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y98UotqGzUGf"
      },
      "source": [
        "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JkZEJjLzjI0"
      },
      "source": [
        "앞서 W와 b는 torch.zeros를 통해 전부 0으로 초기화 된 상태입니다. 이 상태에서 예측값을 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgdzBCO-zhgs"
      },
      "source": [
        "print(hypothesis) # 예측값인 H(x) 출력"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7L0-BztzqNe"
      },
      "source": [
        "실제값 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.\n",
        "\n",
        "사실 가설식을 좀 더 간단하게도 구현할 수 있습니다. 이미 파이토치에서는 시그모이드 함수를 이미 구현하여 제공하고 있기 때문입니다. 다음은 torch.sigmoid를 사용하여 좀 더 간단히 구현한 가설식입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekhAuQk2zk14"
      },
      "source": [
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhgRELmrzwzn"
      },
      "source": [
        "앞서 구현한 식과 본질적으로 동일한 식입니다. 마찬가지로 W와 b가 0으로 초기화 된 상태에서 예측값을 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J033OiRKztXs"
      },
      "source": [
        "print(hypothesis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQUrPL_bz9It"
      },
      "source": [
        "앞선 결과와 동일하게 y_train과 크기가 동일한 6 × 1의 크기를 가지는 예측값 벡터가 나오는데 모든 값이 0.5입니다.\n",
        "\n",
        "이제 아래의 비용 함수값. 즉, 현재 예측값과 실제값 사이의 cost를 구해보겠습니다.\n",
        "\n",
        "$cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]$\n",
        "\n",
        "우선, 현재 예측값과 실제값을 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-LO2N_-z0t2"
      },
      "source": [
        "print(hypothesis)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvBF-Rkn0SLk"
      },
      "source": [
        "현재 총 6개의 원소가 존재하지만 하나의 샘플. 즉, 하나의 원소에 대해서만 오차를 구하는 식을 작성해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emSqCltv0Ho8"
      },
      "source": [
        "-(y_train[0] * torch.log(hypothesis[0]) + \n",
        "  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1N62E380WV_"
      },
      "source": [
        "이제 모든 원소에 대해서 오차를 구해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNmeK2dC0UQS"
      },
      "source": [
        "losses = -(y_train * torch.log(hypothesis) + \n",
        "           (1 - y_train) * torch.log(1 - hypothesis))\n",
        "print(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjqPUKqG0gLD"
      },
      "source": [
        "그리고 이 전체 오차에 대한 평균을 구합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yw__vVR0XnL"
      },
      "source": [
        "cost = losses.mean()\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZdFawoB0j5-"
      },
      "source": [
        "결과적으로 얻은 cost는 0.6931입니다.\n",
        "\n",
        "지금까지 비용 함수의 값을 직접 구현하였는데, 사실 파이토치에서는 로지스틱 회귀의 비용 함수를 이미 구현해서 제공하고 있습니다.\n",
        "사용 방법은 torch.nn.functional as F와 같이 임포트 한 후에 F.binary_cross_entropy(예측값, 실제값)과 같이 사용하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2EsLVd90kY4"
      },
      "source": [
        "F.binary_cross_entropy(hypothesis, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5XzdaXX0oP3"
      },
      "source": [
        "동일하게 cost가 0.6931이 출력되는 것을 볼 수 있습니다. 모델의 훈련 과정까지 추가한 전체 코드는 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y58AL-fu0mC5"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNHAMROJ0pfa"
      },
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "    cost = -(y_train * torch.log(hypothesis) + \n",
        "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwrsnzWr0sCh"
      },
      "source": [
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "print(hypothesis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t1QoKyk0ve4"
      },
      "source": [
        "현재 위 값들은 0과 1 사이의 값을 가지고 있습니다. 이제 0.5를 넘으면 True, 넘지 않으면 False로 값을 정하여 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKx4t0Iw0uH4"
      },
      "source": [
        "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgTsCU9j07Tr"
      },
      "source": [
        "실제값은 [[0], [0], [0], [1], [1], [1]]이므로, 이는 결과적으로 False, False, False, True, True, True와 동일합니다. 즉, 기존의 실제값과 동일하게 예측한 것을 볼 수 있습니다. 훈련이 된 후의 W와 b의 값을 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR_FzSg70xFJ"
      },
      "source": [
        "print(W)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0s2IIrw0-Yz"
      },
      "source": [
        "## 3.2 nn.Module로 구현하는 로지스틱 회귀\n",
        "\n",
        "잠깐만 복습을 해보면 선형 회귀 모델의 가설식은 $H(x)=Wx+b$이었습니다. 그리고 이 가설식을 구현하기 위해서 파이토치의 nn.Linear()를 사용했습니다. 그리고 로지스틱 회귀의 가설식은 $H(x)=sigmoid(Wx+b)$입니다. 파이토치에서는 nn.Sigmoid()를 통해서 시그모이드 함수를 구현하므로 결과적으로 nn.Linear()의 결과를 nn.Sigmoid()를 거치게하면 로지스틱 회귀의 가설식이 됩니다.\n",
        "\n",
        "파이토치를 통해 이를 구현해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW3Fv4t_232o"
      },
      "source": [
        "### 3.2.1 파이토치의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRwmewSJ08wO"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7T3VgNT1KSX"
      },
      "source": [
        "nn.Sequential()은 nn.Module 층을 차례로 쌓을 수 있도록 합니다. 뒤에서 이를 이용해서 인공 신경망을 구현하게 되므로 기억하고 있으면 좋습니다. 조금 쉽게 말해서 nn.Sequential()은 $Wx+b$와 같은 수식과 시그모이드 함수 등과 같은 여러 함수들을 연결해주는 역할을 합니다. 이를 이용해서 로지스틱 회귀를 구현해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APWII4oI1JDz"
      },
      "source": [
        "model = nn.Sequential(\n",
        "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
        "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxg6AoQ81VVa"
      },
      "source": [
        "현재 W와 b는 랜덤 초기화가 된 상태입니다. 훈련 데이터를 넣어 예측값을 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGMKsq_M1Txv"
      },
      "source": [
        "model(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m97rQE9s1ZJl"
      },
      "source": [
        "6 × 1 크기의 예측값 텐서가 출력됩니다. 그러나 현재 W와 b는 임의의 값을 가지므로 현재의 예측은 의미가 없습니다.\n",
        "이제 경사 하강법을 사용하여 훈련해보겠습니다. 총 100번의 에포크를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-_ZL2UX1Wac"
      },
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 20번마다 로그 출력\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
        "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOSJZN2R1gBZ"
      },
      "source": [
        "중간부터 정확도는 100%가 나오기 시작합니다. 기존의 훈련 데이터를 입력하여 예측값을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0TUeYmh1bDS"
      },
      "source": [
        "model(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEvgY9z51jei"
      },
      "source": [
        "0.5를 넘으면 True, 그보다 낮으면 False로 간주합니다. 실제값은 [[0], [0], [0], [1], [1], [1]]입니다. 이는 False, False, False, True, True, True에 해당되므로 전부 실제값과 일치하도록 예측한 것을 확인할 수 있습니다.\n",
        "\n",
        "훈련 후의 W와 b의 값을 출력해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1T7oIM81h32"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j98eODQR1nTl"
      },
      "source": [
        "출력값이 앞 챕터에서 nn.Module을 사용하지 않고 로지스틱 회귀를 구현한 실습에서 얻었던 W와 b와 거의 일치합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8l1hxvK1omE"
      },
      "source": [
        "### 3.2.2 인공 신경망으로 표현되는 로지스틱 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lXKktmX1sWj"
      },
      "source": [
        "사실 로지스틱 회귀는 인공 신경망으로 간주할 수 있습니다.\n",
        "\n",
        "![대체 텍스트](https://wikidocs.net/images/page/58686/logistic_regression.PNG)\n",
        "\n",
        "위의 인공 신경망 그림에서 각 화살표는 입력과 곱해지는 가중치 또는 편향입니다. 각 입력에 대해서 검은색 화살표는 가중치, 회색 화살표는 편향이 곱해집니다. 각 입력 $x$는 각 입력의 가중치 $w$와 곱해지고, 편향 $b$는 상수 1과 곱해지는 것으로 표현되었습니다. 그리고 출력하기 전에 시그모이드 함수를 지나게 됩니다.\n",
        "\n",
        "결과적으로 위의 인공 신경망은 다음과 같은 다중 로지스틱 회귀를 표현하고 있습니다.\n",
        "\n",
        "$H(x)=sigmoid(x_{1}w_{1} + x_{2}w_{2} + b)$\n",
        "\n",
        "뒤에서 인공 신경망을 배우면서 언급하겠지만, 시그모이드 함수는 인공 신경망의 은닉층에서는 거의 사용되지 않습니다.\n",
        "\n",
        "로지스틱 회귀와 소프트맥스 회귀 : https://hyeonnii.tistory.com/239\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wyQwkOm2biX"
      },
      "source": [
        "## 3.3 클래스로 파이토치 모델 구현하기\n",
        "\n",
        "파이토치의 대부분의 구현체들은 대부분 모델을 생성할 때 클래스(Class)를 사용하고 있습니다. 앞서 배운 선형 회귀를 클래스로 구현해보겠습니다. 앞서 구현한 코드와 다른 점은 오직 클래스로 모델을 구현했다는 점입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2JQwgCA3FAN"
      },
      "source": [
        "### 3.3.1 모델을 클래스로 구현하기\n",
        "\n",
        "앞서 로지스틱 회귀 모델은 다음과 같이 구현했었습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BRw0ELj1k_S"
      },
      "source": [
        "model = nn.Sequential(\n",
        "   nn.Linear(2, 1), # input_dim = 2, output_dim = 1\n",
        "   nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rvqTfFn3LO6"
      },
      "source": [
        "이를 클래스로 구현하면 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2639i8fG3KBq"
      },
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHhjPJ5o3Nnc"
      },
      "source": [
        "위와 같은 클래스를 사용한 모델 구현 형식은 대부분의 파이토치 구현체에서 사용하고 있는 방식으로 반드시 숙지할 필요가 있습니다.\n",
        "\n",
        "클래스(class) 형태의 모델은 nn.Module 을 상속받습니다. 그리고 __init__()에서 모델의 구조와 동적을 정의하는 생성자를 정의합니다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으호 호출됩니다. super() 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화 됩니다. foward() 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수입니다. 이 forward() 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행이됩니다. 예를 들어 model이란 이름의 객체를 생성 후, model(입력 데이터)와 같은 형식으로 객체를 호출하면 자동으로 forward 연산이 수행됩니다.\n",
        "\n",
        "\n",
        "\n",
        "*   $H(x)$  식에 입력 $x$로부터 예측된 $y$를 얻는 것을 forward 연산이라고 합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJDVdTeC3WJ7"
      },
      "source": [
        "### 3.3.2 로지스틱 회귀 클래스로 구현하기\n",
        "\n",
        "이제 모델을 클래스로 구현한 코드를 보겠습니다. 달라진 점은 모델을 클래스로 구현했다는 점 뿐입니다. 다른 코드는 전부 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtbhI0ne3Mus"
      },
      "source": [
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]]\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcypMQ0A3bTy"
      },
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m97lD0N33cRp"
      },
      "source": [
        "model = BinaryClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6-YZhlw3dOp"
      },
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    hypothesis = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 20번마다 로그 출력\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
        "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
        "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv0fIT2b3eeO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPsTzurIOflWBeeq/81UgLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namsik-Yoon/pytorch_basic/blob/master/7.%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9S-HrJswJ93",
        "colab_type": "text"
      },
      "source": [
        "# 7. 자연어 처리의 전처리\n",
        "\n",
        "이번 챕터에서는 자연어 처리의 전처리 과정을 이해합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r7tBsCbwSnK",
        "colab_type": "text"
      },
      "source": [
        "## 7.1 자연어 처리 전처리 이해하기\n",
        "\n",
        "자연어 처리는 일반적으로 토큰화, 단어 집합 생성, 정수 인코딩, 패딩, 벡터화의 과정을 거칩니다. 이번 챕터에서는 이러한 전반적인 과정에 대해서 이해합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s36I5L53wYqd",
        "colab_type": "text"
      },
      "source": [
        "### 7.1.1 토큰화(Tokenization)\n",
        "\n",
        "주어진 텍스트를 단어 또는 문자 단위로 자르는 것을 토큰화라고 합니다. 예를 들어 주어진 문장이 다음과 같다고 해봅시다. 영어의 경우 토큰화를 사용하는 도구로서 대표적으로 spaCy와 NLTK가 있습니다. 물론, 파이썬 기본 함수인 split으로 토큰화를 할 수도 있습니다.\n",
        "\n",
        "우선 영어에 대해서 토큰화 실습을 해봅시다.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "en_text = \"A Dog Run back corner near spare bedrooms\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dikDMh4UwgZB",
        "colab_type": "text"
      },
      "source": [
        "1) spaCy 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64-I0eQuwRcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVnzus6jwsZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(en_text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5nssQYZwuhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10386150-f0b5-42e0-a8f2-6e9e65ca97d8"
      },
      "source": [
        "en_text = \"A Dog Run back corner near spare bedrooms\"\n",
        "print(tokenize(en_text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OYciwnMw8k_",
        "colab_type": "text"
      },
      "source": [
        "2) NLTK 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeGfrL50wwSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "21257528-ca8c-44c8-f2a3-518078a62e1f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97WquVeDxAAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "edafbdab-1274-420f-cc94-ec6d8c4bfc89"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(en_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbazO1gFxFWF",
        "colab_type": "text"
      },
      "source": [
        "3) 띄어쓰기로 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JULgvm8OxEI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bacca1bf-3735-4ac3-eca2-fa369375caeb"
      },
      "source": [
        "print(en_text.split())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3kJv5ccxJS1",
        "colab_type": "text"
      },
      "source": [
        "사실 영어의 경우에는 띄어쓰기 단위로 토큰화를 해도 단어들 간 구분이 꽤나 명확하기 때문에, 토큰화 작업이 수월합니다. 하지만 한국어의 경우에는 토큰화 작업이 훨씬 까다롭습니다. 그 이유는 한국어는 조사, 접사 등으로 인해 단순 띄어쓰기 단위로 나누면 같은 단어가 다른 단어로 인식되어서 단어 집합(vocabulary)의 크기가 불필요하게 커지기 때문입니다.\n",
        "\n",
        "\n",
        "\n",
        "*   단어 집합(vocabuary)이란 중복을 제거한 텍스트의 총 단어의 집합(set)을 의미합니다.\n",
        "\n",
        "\n",
        "예를 들어 단어 '사과'가 많이 들어간 어떤 문장에 띄어쓰기 토큰화를 한다면 '사과가', '사과를', '사과의', '사과와', '사과는'과 같은 식으로 같은 단어임에도 조사가 붙어서 다른 단어로 인식될 수 있습니다. 예를 통해 구체적으로 이해해봅시다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUa5_vLjxbgB",
        "colab_type": "text"
      },
      "source": [
        "4) 한국어 띄어쓰기 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_bn6zFgxIQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "99987916-5ad0-461f-a777-ce0b9201360f"
      },
      "source": [
        "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
        "print(kor_text.split())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사과의', '놀라운', '효능이라는', '글을', '봤어.', '그래서', '오늘', '사과를', '먹으려고', '했는데', '사과가', '썩어서', '슈퍼에', '가서', '사과랑', '오렌지', '사왔어']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkPLq_DlxkPw",
        "colab_type": "text"
      },
      "source": [
        "위의 예제에서는 '사과'란 단어가 총 4번 등장했는데 모두 '의', '를', '가', '랑' 등이 붙어있어 이를 제거해주지 않으면 기계는 전부 다른 단어로 인식하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1gCU_x3xt1T",
        "colab_type": "text"
      },
      "source": [
        "5) 형태소 토큰화\n",
        "\n",
        "위와 같은 상황을 방지하기 위해서 한국어는 보편적으로 '형태소 분석기'로 토큰화를 합니다. 여기서는 형태소 분석기 중에서 mecab을 사용해보겠습니다. 아래의 커맨드로 colab에서 mecab을 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2Mz9ixy4w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled14.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnKtHvBR5XzmMUN2G9sO7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Namsik-Yoon/pytorch_basic/blob/master/10.%20%EB%8B%A4%EB%8C%80%EB%8B%A4%20RNN%EC%9D%84%20%EC%9D%B4%EC%9A%A9%ED%95%9C%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l369kWshSuaY"
      },
      "source": [
        "# 10. 다대다 RNN을 이용한 텍스트 생성\r\n",
        "\r\n",
        "이번 챕터에서는 다대다 RNN을 이용하여 텍스트 생성을 해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Q1WCJ9S_Vb"
      },
      "source": [
        "## 10.1 문자 단위 RNN(Char RNN)\r\n",
        "\r\n",
        "이번 챕터에서는 모든 시점의 입력에 대해서 모든 시점에 대해서 출력을 하는 다대다 RNN을 구현해봅시다. 다대다 RNN은 대표적으로 품사 태깅, 개채명 인식 등에서 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rVDgYTDTbqQ"
      },
      "source": [
        "### 10.1.1 문자 단위 RNN(Char RNN)\r\n",
        "\r\n",
        "RNN의 입출력의 단위가 단어 레벨(word-level)이 아니라 문자 레벨(character-level)로 하여 RNN을 구현한다면, 이를 문자 단위 RNN이라고 합니다. RNN 구조 자체가 달라진 것은 아니고, 입, 출력의 단위가 문자로 바뀌었을 뿐입니다. 문자 단위 RNN을 다대다 구조로 구현해봅시다.\r\n",
        "\r\n",
        "우선 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MBprvlfS-aB"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnpFtm0DTjis"
      },
      "source": [
        "1) 훈련 데이터 전처리하기\r\n",
        "\r\n",
        "여기서는 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN을 구현해볼 겁니다. 이렇게 구현하는 어떤 의미가 있지는 않습니다. 그저 RNN의 동작을 이해하기 위한 목적입니다.\r\n",
        "\r\n",
        "입력 데이터와 레이블 데이터에 대해서 문자 집합(voabulary)을 만듭니다. 여기서 문자 집합은 중복을 제거한 문자들의 집합입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QP_HxSETjCq",
        "outputId": "a0240ae7-91f5-4346-965b-20b6ab6e7cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_str = 'apple'\r\n",
        "label_str = 'pple!'\r\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\r\n",
        "vocab_size = len(char_vocab)\r\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25OBRhkvTxW1"
      },
      "source": [
        "현재 문자 집합에는 총 5개의 문자가 있습니다. !, a, e, l, p입니다. 이제 하이퍼파라미터를 정의해줍니다. 이때 입력은 원-핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기여야만 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uos3GKVsTzl2"
      },
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\r\n",
        "hidden_size = 5\r\n",
        "output_size = 5\r\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-VRItuITxYd"
      },
      "source": [
        "이제 문자 집합에 고유한 정수를 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE6zHlbBTwxr",
        "outputId": "cd125c06-7dde-4f8f-f4dc-5ce1404eb3b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\r\n",
        "print(char_to_index)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhq2pLQeT2nW"
      },
      "source": [
        "!은 0, a는 1, e는 2, l은 3, p는 4가 부여되었습니다. 나중에 예측 결과를 다시 문자 시퀀스로 보기위해서 반대로 정수로부터 문자를 얻을 수 있는 index_to_char을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR98lLP0T16G",
        "outputId": "f9c0063b-d7fc-4337-a002-ea4ff4ae1e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "index_to_char={}\r\n",
        "for key, value in char_to_index.items():\r\n",
        "    index_to_char[value] = key\r\n",
        "print(index_to_char)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TligTBiJT6fS"
      },
      "source": [
        "이제 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ctnJVLT4vp",
        "outputId": "c3ee1aed-d0ca-4676-dd3a-bfd68f21aaa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\r\n",
        "y_data = [char_to_index[c] for c in label_str]\r\n",
        "print(x_data)\r\n",
        "print(y_data)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o2gkMnCT9q5"
      },
      "source": [
        "파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받습니다. 그렇기 때문에 배치 차원을 추가해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEZMvxuZT7iO",
        "outputId": "3020b4a5-4f2b-42e3-9e9a-c0e15aefb812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 배치 차원 추가\r\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\r\n",
        "x_data = [x_data]\r\n",
        "y_data = [y_data]\r\n",
        "print(x_data)\r\n",
        "print(y_data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxcqUiDmUA6J"
      },
      "source": [
        "입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viCC4DbrT_KX",
        "outputId": "e3d2b06e-1744-4a38-a8b9-63486009390b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\r\n",
        "print(x_one_hot)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17jYoxp7UDSx"
      },
      "source": [
        "입력 데이터와 레이블 데이터를 텐서로 바꿔줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AV1EZSKUCRu"
      },
      "source": [
        "X = torch.FloatTensor(x_one_hot)\r\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rP38u8GUGjw"
      },
      "source": [
        "이제 각 텐서의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxtqCAwJUE5D",
        "outputId": "0d84f117-ace2-4646-e9be-133468f9d047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\r\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE4Opp52UJjj"
      },
      "source": [
        "2) 모델 구현하기\r\n",
        "\r\n",
        "이제 RNN 모델을 구현해봅시다. 아래에서 fc는 완전 연결층(fully-connected layer)을 의미하며 출력층으로 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aqeLWmiUHu5"
      },
      "source": [
        "class Net(torch.nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size, output_size):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\r\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\r\n",
        "\r\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\r\n",
        "        x, _status = self.rnn(x)\r\n",
        "        x = self.fc(x)\r\n",
        "        return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB3eYXaAUN-E"
      },
      "source": [
        "클래스로 정의한 모델을 net에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lx0sCNbUPP7"
      },
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5lmwmC8UN_4"
      },
      "source": [
        "이제 입력된 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4ep_RsSURxm",
        "outputId": "c4db1160-c3e9-47f3-9f59-a4e3af5b2ac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "outputs = net(X)\r\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sWR0oTvUOB7"
      },
      "source": [
        "(1, 5, 5)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHLzpAmbUXRI",
        "outputId": "e8a2edf9-5de4-41b4-e320-0a0075a04be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981TqzNXUOD1"
      },
      "source": [
        "차원이 (5, 5)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwSpZYXSUaVo",
        "outputId": "132a40d1-571d-4240-dcfa-5747c0ee39f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Y.shape)\r\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sTj0MRsUOF-"
      },
      "source": [
        "레이블 데이터는 (1, 5)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (5)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnzQO3szUc_B"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF6ITutXUOIC"
      },
      "source": [
        "총 100번의 에포크를 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWZKN-cuUf9c",
        "outputId": "b406fb0d-00b9-4144-8d61-3b59d2bc19dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(100):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    outputs = net(X)\r\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\r\n",
        "    loss.backward() # 기울기 계산\r\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\r\n",
        "\r\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\r\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\r\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\r\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss:  1.6186678409576416 prediction:  [[2 3 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eleee\n",
            "1 loss:  1.3273652791976929 prediction:  [[4 3 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pllpp\n",
            "2 loss:  1.0974230766296387 prediction:  [[4 4 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplpp\n",
            "3 loss:  0.9068676233291626 prediction:  [[4 4 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplpp\n",
            "4 loss:  0.7226341962814331 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "5 loss:  0.5407742857933044 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.39408910274505615 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.2832306921482086 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.1974070519208908 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.13430313766002655 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.0893365815281868 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.059174757450819016 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.03984719514846802 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.02761668898165226 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.01979907602071762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.014682700857520103 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.01123257540166378 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.008832665160298347 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.007113558705896139 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.005848965607583523 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.004896798171103001 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.004165076185017824 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.0035926655400544405 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.003137856023386121 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0027715943288058043 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0024728875141590834 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0022266695741564035 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.0020216568373143673 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.001849381485953927 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0017035320634022355 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0015790279721841216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0014720457838848233 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0013795451959595084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0012990072136744857 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0012285783886909485 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0011666660429909825 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0011119864648208022 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0010634695645421743 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.001020187744870782 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0009814989753067493 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0009467847412452102 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0009154740837402642 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0008871626341715455 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0008615171536803246 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0008381331572309136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.000816748826764524 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0007972929743118584 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0007793846307322383 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.000762880954425782 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0007476629689335823 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0007336594280786812 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0007205843576230109 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0007084141252562404 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0006971011171117425 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0006865739705972373 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0006766421720385551 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0006673770258203149 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0006586596136912704 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0006503231707029045 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0006425583851523697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0006351984338834882 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0006281955866143107 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0006215261528268456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0006152139976620674 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.000609092297963798 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0006033278768882155 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0005977063556201756 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0005923943826928735 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.0005872729234397411 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0005822704988531768 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0005775063182227314 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0005728373071178794 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0005684065399691463 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.000564047135412693 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0005598306888714433 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0005557570839300752 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0005517310928553343 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0005478003295138478 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0005441078101284802 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0005403437535278499 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.000536722713150084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0005331967840902507 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0005297661991789937 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0005263355560600758 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0005230001988820732 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0005197125137783587 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0005165438633412123 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0005133990780450404 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0005102541763335466 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0005071807536296546 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0005041788099333644 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0005012483452446759 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0004982939572073519 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0004954349133186042 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0004926234832964838 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0004897643812000751 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.00048700053594075143 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.00048426055582240224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.00048161583254113793 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.00047892340808175504 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBpeG-QKUOKL"
      },
      "source": [
        "## 10.2 문자 단위 RNN(Char RNN) - 더 많은 데이터\r\n",
        "\r\n",
        "이번 챕터에서는 더 많은 데이터 문자 단위 RNN을 구현합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYxbhrTeUOMf"
      },
      "source": [
        "### 10.2.1 문자 단위 RNN(Char RNN)\r\n",
        "\r\n",
        "우선 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQrXYaDhU0ff"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSP89cDtUOOt"
      },
      "source": [
        "다음과 같이 임의의 샘플을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXo_XwY2U6Pm"
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\r\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\r\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5bwyI5LU8S3"
      },
      "source": [
        "문자 집합을 생성하고, 각 문자에 고유한 정수를 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9S2OeOaU9I4",
        "outputId": "54e7b2e4-9151-40f4-9fe7-3d421d7e2576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\r\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\r\n",
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'f': 0, '.': 1, 'a': 2, \"'\": 3, 'c': 4, 'm': 5, 'd': 6, 'u': 7, 'h': 8, 'k': 9, 'n': 10, 'y': 11, 'w': 12, 's': 13, 'i': 14, ',': 15, ' ': 16, 'e': 17, 'p': 18, 'b': 19, 'o': 20, 'l': 21, 'g': 22, 't': 23, 'r': 24}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIcHosDJUOQz"
      },
      "source": [
        "각 문자에 정수가 부여되었으며, 총 25개의 문자가 존재합니다. 문자 집합의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pyuO8nJVBrC",
        "outputId": "78a3e633-5ef4-47a9-8766-6ed4f24203c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dic_size = len(char_dic)\r\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKm8IQ_qUOS7"
      },
      "source": [
        "문자 집합의 크기는 25이며, 입력을 원-핫 벡터로 사용할 것이므로 이는 매 시점마다 들어갈 입력의 크기이기도 합니다. 이제 하이퍼파라미터를 설정합니다. hidden_size(은닉 상태의 크기)를 입력의 크기와 동일하게 줬는데, 이는 사용자의 선택으로 다른 값을 줘도 무방합니다.\r\n",
        "\r\n",
        "그리고 sequence_length라는 변수를 선언했는데, 우리가 앞서 만든 샘플을 10개 단위로 끊어서 샘플을 만들 예정이기 때문입니다. 이는 뒤에서 더 자세히 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAy83aQnVFMG"
      },
      "source": [
        "# 하이퍼파라미터 설정\r\n",
        "hidden_size = dic_size\r\n",
        "sequence_length = 10  # 임의 숫자 지정\r\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X0h-VtwUOVN"
      },
      "source": [
        "다음은 임의로 지정한 sequence_length 값인 10의 단위로 샘플들을 잘라서 데이터를 만드는 모습을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_xn0q0bVIBE",
        "outputId": "ca5a58c4-6ffb-49bf-8342-b252f3177a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 데이터 구성\r\n",
        "x_data = []\r\n",
        "y_data = []\r\n",
        "\r\n",
        "for i in range(0, len(sentence) - sequence_length):\r\n",
        "    x_str = sentence[i:i + sequence_length]\r\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\r\n",
        "    print(i, x_str, '->', y_str)\r\n",
        "\r\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\r\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWOdIXXqUOXC"
      },
      "source": [
        "총 170개의 샘플이 생성되었습니다. 그리고 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태입니다. 첫번째 샘플의 입력 데이터와 레이블 데이터를 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTOmUtg5VOBp",
        "outputId": "70d040b1-0e75-4de4-8f7c-ea7ed8cd97dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(x_data[0])\r\n",
        "print(y_data[0])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14, 0, 16, 11, 20, 7, 16, 12, 2, 10]\n",
            "[0, 16, 11, 20, 7, 16, 12, 2, 10, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOsdVVCNUOZJ"
      },
      "source": [
        "한 칸씩 쉬프트 된 시퀀스가 정상적으로 출력되는 것을 볼 수 있습니다. 이제 입력 시퀀스에 대해서 원-핫 인코딩을 수행하고, 입력 데이터와 레이블 데이터를 텐서로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oBA6JbgUNDZ"
      },
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\r\n",
        "X = torch.FloatTensor(x_one_hot)\r\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1R559zPVQ_U"
      },
      "source": [
        "이제 훈련 데이터와 레이블 데이터의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOXPlPOuVQFM",
        "outputId": "c15218e3-c2d3-4183-e3fa-8721fef4ded1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\r\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODOjIOfFVTke"
      },
      "source": [
        "원-핫 인코딩 된 결과를 보기 위해서 첫번째 샘플만 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwcF9fBVSme",
        "outputId": "dbbb0813-1850-4cfe-be29-56a0cf7ba537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmJAmn9ZVWJd"
      },
      "source": [
        "레이블 데이터의 첫번째 샘플도 출력해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJE6sIyJVVGB",
        "outputId": "a9161125-71b1-49ad-e1cb-c04e9e458aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Y[0])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0, 16, 11, 20,  7, 16, 12,  2, 10, 23])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhy05CzbVYPE"
      },
      "source": [
        "위 레이블 시퀀스는 f you want에 해당됩니다. 이제 모델을 설계합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yif1yfsIW414"
      },
      "source": [
        "2) 모델 구현하기\r\n",
        "\r\n",
        "모델은 앞서 실습한 문자 단위 RNN 챕터와 거의 동일합니다. 다만 이번에는 은닉층을 두 개 쌓을 겁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnfmnM5JVXJW"
      },
      "source": [
        "class Net(torch.nn.Module):\r\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\r\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x, _status = self.rnn(x)\r\n",
        "        x = self.fc(x)\r\n",
        "        return x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8QC4FVjW8RT"
      },
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTmQIEEXW-WH"
      },
      "source": [
        "nn.RNN() 안에 num_layers라는 인자를 사용합니다. 이는 은닉층을 몇 개 쌓을 것인지를 의미합니다. 모델 선언 시 layers라는 인자에 2를 전달하여 은닉층을 두 개 쌓습니다. 비용 함수와 옵티마이저를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dut2if4MW9NY"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS890cnUXCru"
      },
      "source": [
        "이제 모델에 입력을 넣어서 출력의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRn7WBjTW_sd",
        "outputId": "a663c8e7-135c-49df-e782-b9103624698b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "outputs = net(X)\r\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLQvuu0uXFe6"
      },
      "source": [
        "(170, 10, 25)의 크기를 가지는데 각각 배치 차원, 시점(timesteps), 출력의 크기입니다. 나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-IbEgaLXBAE",
        "outputId": "b2f5e18d-8790-4f6e-dc24-22b509a967fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1700, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrK6DALpXHVK"
      },
      "source": [
        "차원이 (1700, 25)가 된 것을 볼 수 있습니다. 이제 레이블 데이터의 크기를 다시 복습봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ic8DxPtXGgd",
        "outputId": "0c7e5abe-5823-406e-b3f6-54f6acb8e901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(Y.shape)\r\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ek1kdLnXJju"
      },
      "source": [
        "레이블 데이터는 (170, 10)의 크기를 가지는데, 마찬가지로 나중에 정확도를 측정할 때는 이걸 펼쳐서 계산할 예정입니다. 이 경우 (1700)의 크기를 가지게 됩니다. 이제 옵티마이저와 손실 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_PASNsaXIix",
        "outputId": "8c271bbc-c90d-48e0-99eb-93182742aada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(100):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\r\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # results의 텐서 크기는 (170, 10)\r\n",
        "    results = outputs.argmax(dim=2)\r\n",
        "    predict_str = \"\"\r\n",
        "    for j, result in enumerate(results):\r\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\r\n",
        "            predict_str += ''.join([char_set[t] for t in result])\r\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\r\n",
        "            predict_str += char_set[result[-1]]\r\n",
        "\r\n",
        "    print(predict_str)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccbcccccccccccccccccccccmccccccccccccmccccccccccccccccccccccccccccccccccccccccbcccccccccccccccmcccccccmcccccccccccccc\n",
            "                                t                         t                       t                             t     t    t                      t                                \n",
            "ttt  tt   t  tt t t t t t t tt t  t t  tt    tttt t t  tttt t t t t  t  tt t  t t tt  t ttt  t t t tt  tt t  t tt t   t t t t t  t t t   tt  tt  tt ttt t tt  t t t t t tt  t t t t\n",
            "  o.g  tu.k.k.k..k.k.k.k.k.k....k..k.k.k..ukk.k..k.k.kkk.k.k.k.k.k..kk.kk..kkk..kk.m.k.k.k.kk..kk.k..k.kk.u.k.kt.k...k.k.k.kkk..u.k..kc.kk.k.k..kk..k..k.k..k.k.kk...kkkkk.k.k.kk.k\n",
            "   t   t                                     t                    t                               t     t         t   t t          t                                               \n",
            "    er te                                                                                                                                                                          \n",
            "     u  oo                            o           o o  o                             t           oo     o    t   t     t                     o       o           oo      oo   o    \n",
            "  t   t t                                                                                                                                            o                             \n",
            "  t     t                                                                                                                                                                          \n",
            "           tttt  t                       t    t t           t       tt t                     t     t                     t   t       t  t       t       t  tt                   tt \n",
            "     t t        tt   n    t       t          t         t    t                   n t             t     t     t          t           t          t     t t            t       t       \n",
            "       t  t t tt  t tntt t    tt     t t  tt  t tt t  t   tt  tt   ttt ttt  ttt t  tt  t tt  tt    ttt   tt     t   t      tt  tt   tt    tt   tt t     t   t t       t        t t \n",
            "  t   t              n                                                                     t                                                   n                                   \n",
            "  t   t                                                       o                            t                                                                                       \n",
            "  t               e         e    o     e                 e    o          eee  t  ee  ee           ee       e      e     e          e   e           e  e   e             e  eoe     \n",
            "  te   t e t e  e e  r  e        o  e ete   e ee  e    e e   eo   e  t   eee     eeoteed e e    e e    e d e    t e   eee e   e  tee  eeee    ee   ee e  ee   e         eeetee    e\n",
            "  to   t e t    e e   t e   to  eo  e  te     ee  e  e e ed eeo  tod e    te      totee   ee t  t e   t    e    t e t  ee et     t e  to e   te    ee e  te  te        tt  t e    e\n",
            "  to   t   t        t t e   t   toe    t      e        eo t  eo  dot t    eo      e t e  o   t  t     tn   o    t       e  t     t t  to e   ee   to  e  e   te     e   to to o   e\n",
            "t t   tn d t          t         t eo t to            m    t  to ed   t    eo  t   e t    o   t  t     eo   o o  t   toe em to    t tm to t    t   to     t    e        tto t  t  m \n",
            "t t   t ed t     t   ot     t  mtoendt to t          m sm t  to e   mt    to  t   e tn  t    e  tn  ttto  to    to  to  sm tome  tnem to t m ntot to     et  tee    e  tto tn      \n",
            "t t   t  d t  aot   t t e   to  toeo t to t    em,   r sm t  to e  p tm   to  t   t tntetota e  tn  t t   to  t t   t t sm to e  tnem to t  dnt   t e e  etnt       e  een dne   mt\n",
            "t to  eo dsto to  t tntneh  toe theoet to tm  d hto  s e  to to ts  heo   tn  to  eoto eeo'tol  tn  t th  to  t tot t ehsm to t et em en em d e   toeneo  eneti     et etn t ente t\n",
            "t lo deo d tose  se tnt em  toe'eheoet to'eo d  otos   e  t  tonta epeo o tod eo  eoeo et  tneo toe p tn  ths e tot eothsm to e seoem eo tm d t   t e tot e eie      t etm toe d e \n",
            "t lo dio   to e g e tneoem  to  toeoet toseo   e tor   e  t  tonse toeo   tnd to 'toeo  r dt e  to  t eo  ton t eo  e them to er ehem eo emn' t   the t t eheie   n  t  to ehe toes\n",
            "t to dion  thneot e totoem, ton toeo t to tr n   tor   e  t  tonsentoeo   tnd ton toeo  andtoep thnp  to  ton d tot e the  t  to ehem t  to   ton t e t d e   t   n et  th eoentor \n",
            "t to  iong th tot e totoem, ton thto t to tr n n tor t er t  tonsrnt tor  tnd ton'toto  andtoer thnpd to  torcd tot e th   to rr them th ton  ton thent d tn  tm  n et  th thert r \n",
            "t to  tnng th tot s tntoem  t n totret to th n   to  t er to tonse t tor  tnd ton toto  an toer thems to  ton t t t eother to ch them th ton  ton the t d tn  am  n et  th th ns r \n",
            "t to  eont th tot s tneoem, t n totoem to th r  nto  ther to tonseng to   tnd ton'toto  en toem toems tn  ton t t t tothem to ch them th ton  ton th mt d tm sam en eth th thens en\n",
            "t to  tont th toa s antoem, ton tothem to ti g emto  ther tonto tent too  tnd to 'toto  an toem toems an  ton t t t eother to cm them to tong tos themt d tm  am enseth tm thens eo\n",
            "t to  tong th t als antmem, tos'toehet to t, r emto  thes to tm teet eoo  tnd aon't ao ean them toems and aos t t t eother to cm them to tong tos ,heme d tms am enseth ao t,ers e \n",
            "t to  tong th t ,le antmem, t s'toehem tr t, n ento  eher to th eent eo   tnd aon't eo  ,n them thems and aon t t t e ther toech them th tong tor therend tss am ens ty eo t,ens e \n",
            "t to  tons th toels tnthep, t n'toehet th tong ento  ther to to eent eon  tnd aon't eo e,n them thems wod ton t t t e ther toech them th tont tor themend tns ap ln ety ao t,ere sn\n",
            "t to  toet th cotls wnehep, ton't ehem tr c,ng entor ther to to  ent eoo  tod aon't eo e,n them tonms wod aorct t toe ther toech them th con  dor therend es  ep en  ty io there nn\n",
            "t to  aont th tutls dnehep, dor't er t tr oentlento  ther to co lect woo  tnd aon't wo o,n them tonps wnd aorct t d w ther to ch them th con  dor ther nd ens ep ens ty io ther  n \n",
            "t th  aont th tutls wnehem, ton't er m ts tenglento  ther to bonlect tor  tnd aon't ws i,n them tonks wnd aonc, todhw ther to ch them th bont dor therend tns op ensity ih thems n \n",
            "m tr  aont wh butls wnihep, ton't whem ts bennsenao  ther to bo lent too  and aon't ws i,n them tonks and aorkt t t wnther toech them to bong tor thertnd tns ep insity er thems nc\n",
            "m th  aont th luels anerem, don't whem tp teenlento  ther to bonlect toog and aon't ws i,n them tonks and aork, aot w ther toech them to lont dor thertnd ens ip insity ir thems s \n",
            "m tr  aant thgbuils ansmim, don't eh m tm ceotlento  ther to bonlect too  and aon't eo i,n them tonks and aor , dodow ther toech them to bont dor therin  ensmim ensity ef the s s \n",
            "m tr  aan  th buils andhim, don't ehum tp pentlento  ther to bonlect woog and aon't eo i,n them tonks and aor , dotoa ther toech them to bong dor therend ess im ensety ef the s n \n",
            "m th oaant th luile anihim, don't ahum tp leotlento  ther to lonlent wood and aon't aosi,n them tonks and aork, a t w ther toech them to long dor therendless epmensety ef the s  n\n",
            "m eh taant to luile anirim, don't arum ep lentlento  ther to lonlect wood and aon't assign them tonks and aork, d t wnther toech them to lond dor therindlesssiimen ity ef the sn n\n",
            "myih oaant to luils anihip, don't arut ep leotlento  ther to loelect aood and aon't assegn ther tonps and aork, d t wather to ch them to lond dor there dless emmensity eo the s sn\n",
            "m io taant do cuile ani im, don't wrum up leotlento  ther to lonlect woos and aon't assign them tosks and aor , d t w ther toach them to long for theremdlenssemmensity ef the s sc\n",
            "m io maant to cuile anshim, don't rrum up panplecto  ther t  lollect wood and aon't assign them tosps an  aork, b t rather toach them to long for therendless emmensity eo the s sn\n",
            "m io meant to cuile anship, don't rrum up peetlemto  ther t  co lect wood and aon't assign them tosps and aork, but rather t ach them th long bor therendless emmensity ef the s  c\n",
            "m io  aant to cuil  anship, don't arum up peotlemto  ther t acollect wood and aon't assign them tosks and aork, but rather t ach them to long for therendless emmensity ef thers ic\n",
            "m io  aant to cuil  anship, don't arum up peotlecto  ther to lollect wood and aon't assign them tosks and aork, but rather toach them to long for therendless emmensity ef the s in\n",
            "m io  eant to cuild anship, don't arum up eeonlecto  ther to lollest wood and aon't assign them tosks and aork, but rather t ach them to long dor therendless immensity ef the s cn\n",
            "m io  aant to cuild anship, don't arum up peoplecto  ther toalollect wood and aon't assign them tosks and dork, but rather t ach them to long dor the endless immensity of the t cc\n",
            "m io  aant to cuild anship, don't drum up peeple tog ther t  collect wood and don't assign them tosks and dork, but rather t ach them to long for the endless immensity of the s cc\n",
            "m oo  aant to cuild anship, don't drum up people tog ther t  lollect wood and don't assign them tosks and dork, but rather t ach them to long for the s dless immensity of the s cc\n",
            "m oou aont to cuild anship, don't drum up peoplectog ther t dlollect wood and don't assign them tosks and dork, dut rather t ach them to long for the endless immensity of the ssct\n",
            "m io  aant to cuild anship, don't arum up people bog ther to lollect wood and don't assign them tosks and aork, but rather t ach them to long for the endless immensity of the s cc\n",
            "m io  aant to cuild anship, don't drum up people tog ther toacollect wood and don't assign them tasks and dork, but rather t gch them to long for the endless immensity of the secc\n",
            "m io  aant to cuild anship, don't drum up peoplectog ther toacollect wood and don't assign them tasks and dork, but rather to ch them to long for the endless immensity of themsecc\n",
            "m iou aant do cuild anship, don't rrum up people tog ther toacollect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of themsecc\n",
            "m io  aant to luild anship, don't arum up people tog ther toalollect wood and don't assign them tasps and dork, but rather teach them to long for the endless immensity of the s nc\n",
            "m iou aant to luild anship, don't arum up people together t  bollect wood and don't asskgn them tasks and dork, dut rather t ach them to long for the esdless immensity of the s uc\n",
            "t o u aant to luild anship, don't arum up people together to lollect wood and don't assign them tosks and dork, dut rather t ach them to long for the esdless immensity of the s uc\n",
            "t o u aont to build anship, don't drum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the end ess immensity of the s uc\n",
            "t iou aant to cuild anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s nc\n",
            "t iou aant to build anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the s cc\n",
            "t iou aant to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s cc\n",
            "t iou aant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather te ch them to long for the endless immensity of the senc\n",
            "t iou aant to luild a ship, don't drum up people toaether teacollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s ac\n",
            "m iou aant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the s ak\n",
            "m iou aant to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the s at\n",
            "m iou aant to build anship, don't drum up people together toacollect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seac\n",
            "m you aant to build anship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the senk\n",
            "m you aant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
            "m you aant to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the s  .\n",
            "m you aant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the se .\n",
            "m you aant to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you aant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1kc2G-XQPj"
      },
      "source": [
        "처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성하는 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPq4-1loXSY5"
      },
      "source": [
        "## 10.3 단어 단위 RNN - 임베딩 사용\r\n",
        "\r\n",
        "이번 챕터에서는 문자 단위가 아니라 RNN의 입력 단위를 단어 단위로 사용합니다. 그리고 단어 단위를 사용함에 따라서 Pytorch에서 제공하는 임베딩 층(embedding layer)를 사용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQZS3dFcXce0"
      },
      "source": [
        "1) 훈련 데이터 전처리하기\r\n",
        "\r\n",
        "우선 실습을 위해 필요한 도구들을 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uikq4ROXXK0m"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycdY_YgoXhdp"
      },
      "source": [
        "실습을 위해 임의의 문장을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHUvqwEyXgjk"
      },
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VygBCHeXjlc"
      },
      "source": [
        "우리가 만들 RNN은 'Repeat is the best medicine for'을 입력받으면 'is the best medicine for memory'를 출력하는 RNN입니다. 위의 임의의 문장으로부터 단어장(vocabulary)을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFCjmYx0Xiq3",
        "outputId": "fdb8addc-98ee-4a62-c780-555dfc72f396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = list(set(sentence))\r\n",
        "print(vocab)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Repeat', 'medicine', 'the', 'for', 'memory', 'best', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsQv7mQEXnVF"
      },
      "source": [
        "이제 단어장의 단어에 고유한 정수 인덱스를 부여합니다. 그리고 그와 동시에 모르는 단어를 의미하는 UNK 토큰도 추가하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USjTqMdNXlFg",
        "outputId": "02c33fe6-8f6b-41f9-dd63-9c69368a8171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\r\n",
        "word2index['<unk>']=0\r\n",
        "print(word2index)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Repeat': 1, 'medicine': 2, 'the': 3, 'for': 4, 'memory': 5, 'best': 6, 'is': 7, '<unk>': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6KEtIFtXroj"
      },
      "source": [
        "이제 word2index가 우리가 사용할 최종 단어장인 셈입니다. word2index에 단어를 입력하면 맵핑되는 정수를 리턴합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUUdQX1cXoxU",
        "outputId": "8a8c1027-aae0-4341-a53d-bb6936c443cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(word2index['memory'])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODVZ02QZXvKm"
      },
      "source": [
        "단어 'memory'와 맵핑되는 정수는 2입니다. 예측 단계에서 예측한 문장을 확인하기 위해 idx2word도 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6eYX9tgXs19",
        "outputId": "0b3a7b0c-7ec3-453e-c2f6-f628b908346c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 수치화된 데이터를 단어로 바꾸기 위한 사전\r\n",
        "index2word = {v: k for k, v in word2index.items()}\r\n",
        "print(index2word)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'Repeat', 2: 'medicine', 3: 'the', 4: 'for', 5: 'memory', 6: 'best', 7: 'is', 0: '<unk>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGe5JjKKXxv_"
      },
      "source": [
        "idx2word는 정수로부터 단어를 리턴하는 역할을 합니다. 정수 2를 넣어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCKf3y9VXwXI",
        "outputId": "efac0574-fc7d-4fe4-cdd3-0535ea35a168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(index2word[2])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "medicine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFO88aYEX0yC"
      },
      "source": [
        "정수 2와 맵핑되는 단어는 medicine인 것을 확인할 수 있습니다. 이제 데이터의 각 단어를 정수로 인코딩하는 동시에, 입력 데이터와 레이블 데이터를 만드는 build_data라는 함수를 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oRACWxbXzci"
      },
      "source": [
        "def build_data(sentence, word2index):\r\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \r\n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\r\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\r\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\r\n",
        "    return input_seq, label_seq"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSMBT8dxX6aq"
      },
      "source": [
        "만들어진 함수로부터 입력 데이터와 레이블 데이터를 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAwdreOHX5Z-",
        "outputId": "cb60ceea-c098-461a-d83a-22bb4bafc60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X, Y = build_data(sentence, word2index)\r\n",
        "print(X) # Repeat is the best medicine for을 의미\r\n",
        "print(Y) # is the best medicine for memory을 의미"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 7, 3, 6, 2, 4]])\n",
            "tensor([[7, 3, 6, 2, 4, 5]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4cIA9rzYFeQ"
      },
      "source": [
        "2) 모델 구현하기\r\n",
        "\r\n",
        "이제 모델을 설계합니다. 이전 모델들과 달라진 점은 임베딩 층을 추가했다는 점입니다. 파이토치에서는 nn.Embedding()을 사용해서 임베딩 층을 구현합니다. 임베딩층은 크게 두 가지 인자를 받는데 첫번째 인자는 단어장의 크기이며, 두번째 인자는 임베딩 벡터의 차원입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_SFnUTGX7zB"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\r\n",
        "                                            embedding_dim=input_size)\r\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\r\n",
        "                                batch_first=batch_first)\r\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # 1. 임베딩 층\r\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\r\n",
        "        output = self.embedding_layer(x)\r\n",
        "        # 2. RNN 층\r\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\r\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\r\n",
        "        output, hidden = self.rnn_layer(output)\r\n",
        "        # 3. 최종 출력층\r\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\r\n",
        "        output = self.linear(output)\r\n",
        "        # 4. view를 통해서 배치 차원 제거\r\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\r\n",
        "        return output.view(-1, output.size(2))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td3lMtoyYJxw"
      },
      "source": [
        "이제 모델을 위해 하이퍼파라미터를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJqbI_8rYL1P"
      },
      "source": [
        "# 하이퍼 파라미터\r\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\r\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\r\n",
        "hidden_size = 20  # RNN의 은닉층 크기"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVHZ_759YJ0h"
      },
      "source": [
        "모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFvU479tYOvb"
      },
      "source": [
        "# 모델 생성\r\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "# 손실함수 정의\r\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\r\n",
        "# 옵티마이저 정의\r\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ3ir_JtYJ2f"
      },
      "source": [
        "모델에 입력을 넣어서 출력을 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDzu0mW7YRvJ",
        "outputId": "348ece00-a5e6-4255-c10a-36d9320d36ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\r\n",
        "output = model(X)\r\n",
        "print(output)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2873, -0.0615, -0.0628, -0.2206, -0.0061, -0.0161,  0.2221,  0.2344],\n",
            "        [-0.3845, -0.0020,  0.0817, -0.1301, -0.0404, -0.3415, -0.0429,  0.0429],\n",
            "        [-0.1288,  0.1987, -0.0587, -0.3164, -0.0085, -0.1632,  0.1932,  0.2772],\n",
            "        [-0.2759,  0.2059, -0.0301, -0.2422, -0.1110, -0.2307, -0.0939,  0.0695],\n",
            "        [-0.2075, -0.1662,  0.3658, -0.3199, -0.0507, -0.3711, -0.1104,  0.5501],\n",
            "        [ 0.1601,  0.0543,  0.0007, -0.0766,  0.2134, -0.3685,  0.2552, -0.0581]],\n",
            "       grad_fn=<ViewBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5AJvNuBYJ4c"
      },
      "source": [
        "모델이 어떤 예측값을 내놓기는 하지만 현재 가중치는 랜덤 초기화되어 있어 의미있는 예측값은 아닙니다. 예측값의 크기를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ8lH6_vYVn6",
        "outputId": "0628d38b-d383-430e-8a36-7d5e4589fdd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(output.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVnfoWM5YJ6d"
      },
      "source": [
        "예측값의 크기는 (6, 8)입니다. 이는 각각 (시퀀스의 길이, 은닉층의 크기)에 해당됩니다. 모델은 훈련시키기 전에 예측을 제대로 하고 있는지 예측된 정수 시퀀스를 다시 단어 시퀀스로 바꾸는 decode 함수를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E89UqXnXYYQp"
      },
      "source": [
        "# 수치화된 데이터를 단어로 전환하는 함수\r\n",
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIioobaYYJ-n"
      },
      "source": [
        "약 200 에포크 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft47-bEyYcFC",
        "outputId": "f4eaed81-1c71-4913-bd07-61e8746e4586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 훈련 시작\r\n",
        "for step in range(201):\r\n",
        "    # 경사 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # 순방향 전파\r\n",
        "    output = model(X)\r\n",
        "    # 손실값 계산\r\n",
        "    loss = loss_function(output, Y.view(-1))\r\n",
        "    # 역방향 전파\r\n",
        "    loss.backward()\r\n",
        "    # 매개변수 업데이트\r\n",
        "    optimizer.step()\r\n",
        "    # 기록\r\n",
        "    if step % 40 == 0:\r\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\r\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\r\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\r\n",
        "        print()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[01/201] 2.0870 \n",
            "Repeat is medicine is Repeat is best\n",
            "\n",
            "[41/201] 1.4152 \n",
            "Repeat is the best medicine for best\n",
            "\n",
            "[81/201] 0.7414 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.3596 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.1951 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1205 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}